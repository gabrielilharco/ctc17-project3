{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Movie Rates with Decision Tree\n",
    "\n",
    "In this report, we are building step by step a decision tree to predict user ratings on movies using the MovieLens dataset, available at http://grouplens.org/datasets/movielens/\n",
    "\n",
    "The dataset is composed by 3 main files:\n",
    "\n",
    "**movies.dat:** this file contains information of all movies, in the format < movie id > :: < movie name > :: < pipe separeted list of genders >\n",
    "\n",
    "**users.dat:** this file contains information of all users, in the format < user id > :: < user gender > :: < user age > :: < ocupation > :: < zip code > \n",
    "\n",
    "**ratings.dat:** this file contains information of all ratings, in the format < user id > :: < movie id > :: < rating > :: < timestamp >\n",
    "\n",
    "\n",
    "For the purpose of this project, **20 ratings were added** to the dataset, based on the personal ratings of the authors of this paper. They correspond to all ratings from users with id 6041 and 6042. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "\n",
    "### Building data frame\n",
    "\n",
    "The first step in this project is to join the data in the 3 data files into a feature and a label matrices. It is very important to carefully choose the attributes to build and train the tree.\n",
    "\n",
    "The function **build_features** does all the processing, and returns the dataframe we will be using. The function **get_genre_id** takes a list of genres and returns its corresponding id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features. This may take a while...\n",
      "Done. Showing 5 first rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>ocupation</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978298413</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>978220179</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>32793</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>978199279</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>22903</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978158471</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>95350</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id movie_id  rating  timestamp gender  age ocupation zip_code genre\n",
       "0       1     1193       5  978300760      F    1        10    48067    64\n",
       "1       2     1193       5  978298413      M   56        16    70072    64\n",
       "2      12     1193       4  978220179      M   25        12    32793    64\n",
       "3      15     1193       4  978199279      M   25         7    22903    64\n",
       "4      17     1193       5  978158471      M   50         1    95350    64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "def get_genre_id(genres_list):\n",
    "    \"\"\"\n",
    "    return the integer id of a list of genres, \n",
    "    which is the integer corresponding to the binary intersection of the list with all the genres\n",
    "    \"\"\"\n",
    "    # movie genres\n",
    "    genres = [\n",
    "        'Action',\n",
    "        'Adventure',\n",
    "        'Animation',\n",
    "        'Children\\'s',\n",
    "        'Comedy',\n",
    "        'Documentary',\n",
    "        'Drama',\n",
    "        'Fantasy',\n",
    "        'Film-Noir',\n",
    "        'Horror',\n",
    "        'Musical',\n",
    "        'Mystery',\n",
    "        'Romance',\n",
    "        'Sci-Fi',\n",
    "        'Thriller',\n",
    "        'War',\n",
    "        'Western'\n",
    "    ]\n",
    "    # creating id\n",
    "    id = 0\n",
    "    weight = 1\n",
    "    for genre in genres:\n",
    "        if genre in genres_list:\n",
    "            id += weight\n",
    "        weight*=2\n",
    "    return str(id)\n",
    "    \n",
    "\n",
    "def build_features(columns=None):\n",
    "    \"\"\"\n",
    "    read data from movies, users and rating and return a single pandas dataframe\n",
    "    of the joined tables, containing all info on ratings.\n",
    "    If columns is passed, only those columns are selected.\n",
    "    \"\"\"\n",
    "    print \"Building features. This may take a while...\"\n",
    "    # reading movies data\n",
    "    movies_df = pd.DataFrame(columns=['movie_id', 'genre'])\n",
    "    with open('../data/movies.dat','r') as file:\n",
    "        lines = file.readlines()\n",
    "        for idx in range(len(lines)):\n",
    "            row = lines[idx].split(\"::\")\n",
    "            movie_genres = row[-1][:-1].split('|')\n",
    "            row = row[:-2] # ignore genre and movie name\n",
    "            row.append(get_genre_id(movie_genres)) # append genre id\n",
    "            movies_df.loc[idx] = row\n",
    "    \n",
    "    # reading users data\n",
    "    users_df = pd.read_table('../data/users.dat', \n",
    "                    names=['user_id', 'gender', 'age', 'ocupation', 'zip_code'], \n",
    "                     sep='::', engine='python')\n",
    "    users_df['user_id'] = users_df['user_id'].apply(lambda x: str(x))\n",
    "    users_df['ocupation'] = users_df['ocupation'].apply(lambda x: str(x))\n",
    "    \n",
    "    # reading ratings data\n",
    "    ratings_df = pd.read_table('../data/ratings.dat', \n",
    "                    names=['user_id', 'movie_id', 'rating', 'timestamp'], \n",
    "                    sep='::', engine='python')\n",
    "    ratings_df['movie_id'] = ratings_df['movie_id'].apply(lambda x: str(x))\n",
    "    ratings_df['user_id'] = ratings_df['user_id'].apply(lambda x: str(x))\n",
    "    \n",
    "    # join tables\n",
    "    user_ratings_df = ratings_df.merge(users_df, how='inner', on='user_id')\n",
    "    features_df = user_ratings_df.merge(movies_df, how='inner', on='movie_id')\n",
    "    \n",
    "    # drop unwanted features\n",
    "    if columns is not None:\n",
    "        features_df = features_df[columns]\n",
    "\n",
    "    # print first 5 rows\n",
    "    print \"Done. Showing 5 first rows:\"\n",
    "    display(features_df.head()) \n",
    "    \n",
    "    return features_df\n",
    "    \n",
    "data = build_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing features\n",
    "\n",
    "To better understand our data, let's take a look at the number of distinct values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for feature in data.columns:\n",
    "    print \"Number of distinct values of field \" + feature + \":\",\n",
    "    print str(len(data[feature].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first approach, we will try to predict the rating without using explicitely the id of the movie and the id of the user. This is because, although they could be good criteria, the large amount of different values in those columns represent a great computational cost. The same goes to the user's zip code and timestamps. For those features, it is unlikely that they wield much relevant information. Finally, we will be using the genres of the movies. There are 17 genres (Action, Adventure, Animation, Children, Comedy, Documentary, Drama, Fantasy, Film-Noir, Horror, Musical, Mystery, Romance, Sci-Fi, Thriller, War and Western). We will store all that information in a single number, build with the binary representation of the movies genres. Since there are 17 different genres, we will be storing numbers from 0 to 2^17-1 = 131071. In practice, we expect a lot less different genre values (in fact, we get only 263 different values). Note that should give as a decision tree with about 2 \\* 7 \\* 21 \\* 263 = 77322, which seems reasonable. We will now proceed to create the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation/Test split\n",
    "\n",
    "We now construct a function to split our dataset into three different sets:\n",
    "\n",
    "- A training set, which will be used to build the classifiers\n",
    "- A validation set, which will be used to determine the best hyperparameters of the classifiers (for example, the maximum depth of the three)\n",
    "- A test set, which will be used to measure the performance\n",
    "\n",
    "We will randomly construct the sets, and we will spit the sets with 60%, 20% and 20% of the shuffled data, respectively for the trainig, validation and test sets. Since decision trees are very prone to overfit, it is very important we get **stratified samples** from the population. The function **dataset_split** takes care of that, keeping the ratio between classes approximately the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataset_split(df, label_name, train_size=0.6, validation_size=0.2):\n",
    "    \"\"\"\n",
    "    splits the data into training, validation and test sets\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty dataframes\n",
    "    train_set = pd.DataFrame(columns=list(df.columns))\n",
    "    validation_set = pd.DataFrame(columns=list(df.columns))\n",
    "    test_set = pd.DataFrame(columns=list(df.columns))\n",
    "    \n",
    "    # stratify sampling\n",
    "    labels = df[label_name].unique()\n",
    "    for label in labels:\n",
    "        label_df = df.ix[df[label_name] == label]\n",
    "        label_df = label_df.sample(frac=1) #randomize\n",
    "        size = len(label_df.index)\n",
    "        validation_start = int(train_size*size)\n",
    "        test_start = int((train_size+validation_size)*size)\n",
    "        train_set = pd.concat([train_set, label_df[:validation_start]])\n",
    "        validation_set = pd.concat([validation_set, label_df[validation_start:test_start]])\n",
    "        test_set = pd.concat([test_set, label_df[test_start:]])\n",
    "    \n",
    "    # randomizing\n",
    "    train_set = train_set.sample(frac=1)\n",
    "    validation_set = validation_set.sample(frac=1)\n",
    "    test_set = test_set.sample(frac=1)\n",
    "    \n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "train_set, validation_set, test_set = dataset_split(data,'rating')\n",
    "\n",
    "print \"Training set size: \" + str(len(train_set.index))\n",
    "print \"Validation set size: \" + str(len(validation_set.index))\n",
    "print \"Test set size: \" + str(len(test_set.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Tree\n",
    "\n",
    "\n",
    "### Entropy\n",
    "\n",
    "We need to create a method for evaluating the homogenity of the set. We will use the standard entropy function for that. \n",
    "\n",
    "For that, it will be usefull to have a function that, given a dataframe, count how many rows are there of each label. Let's construct that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_counts(df, label_name):\n",
    "    \"\"\"\n",
    "    returns the count of each label value\n",
    "    \"\"\"\n",
    "    return df[label_name].value_counts()\n",
    "    \n",
    "print \"Printing how many ratings are for each number of stars:\"\n",
    "counts = label_counts(data, 'rating')\n",
    "for rating, count in counts.iteritems():\n",
    "    print \"Number of \", rating, \"starts ratings:\", str(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to our entropy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log \n",
    "log2 = lambda x: log(x)/log(2)\n",
    "\n",
    "def entropy(df, label_name):\n",
    "    \"\"\"\n",
    "    returns the entropy in a dataset\n",
    "    \"\"\"\n",
    "    s = 0.0\n",
    "    if len(df.index) == 0:\n",
    "        return s\n",
    "    \n",
    "    counts = label_counts(df, label_name)\n",
    "    size = len(df.index)\n",
    "    for _, count in counts.iteritems():\n",
    "        p = float(count)/size\n",
    "        s -= p*log2(p)\n",
    "    return s\n",
    "\n",
    "print \"Initial entropy is:\", entropy(data, 'rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "\n",
    "Another usefull method for building our decision tree classifiers is one that, given some data rows, a feature and a list of values, splits the data into disjoint subsets, according to the value. We will build that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_df(df, feature, values):\n",
    "    \"\"\"\n",
    "    splits a data frame into separate subsets, one for each value\n",
    "    each one corresponding to all the data which has df[feature] == value\n",
    "    \"\"\"\n",
    "    subsets = {}\n",
    "    for value in values:\n",
    "        local_df = df.ix[df[feature] == value]\n",
    "        subsets[value] = local_df\n",
    "        \n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a rule of thumb for preditions\n",
    "\n",
    "Here we define a rule to best predict the outcome of a dataset, if no other information is provided. Here we use the average of all the labels in the dataset, since that minimizes the squared error on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(counts):\n",
    "    \"\"\"\n",
    "    returns the prediction given the count of each label value\n",
    "    \"\"\"\n",
    "    if len(counts) == 0:\n",
    "        # return random number\n",
    "        return np.random.randint(5)+1\n",
    "    pred = 0.0\n",
    "    total = 0.0\n",
    "    for rating, count in counts.iteritems():\n",
    "        pred += count*rating\n",
    "        total += count\n",
    "    return int(pred/total + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the tree\n",
    "\n",
    "Now we start creating the tree. We use a recursive approach, building a node which maximizes the information gain at each step. Note that a node is represented by 3 attributes: the name of the feature, a list of the number of ratings for each class and a dictionary of the children nodes, where the keys are the values assumed by the feature. These variables are passed as parameters to the split_df method above written, dividing the dataset into disjoint subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class node:\n",
    "    \"\"\"\n",
    "    Tree node\n",
    "    \"\"\"\n",
    "    def __init__(self, feature, counts, children={}):\n",
    "        self.feature = feature\n",
    "        self.children = children\n",
    "        self.counts = counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TreeClassifier():\n",
    "    \"\"\"\n",
    "    Decision Tree classifier\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "        \n",
    "    def find_best_criteria(self, df, label_name):\n",
    "        \"\"\"\n",
    "        build a node with a feature and a value \n",
    "        that maximizes information gain for the given dataframe\n",
    "        \"\"\"\n",
    "        # initial variables\n",
    "        best_criteria = None\n",
    "        best_gain = 0.0\n",
    "        best_sets = {}\n",
    "        current_entropy = entropy(df, label_name)\n",
    "\n",
    "        # iterate through all possible choices and select the one with the biggest information gain\n",
    "        for feature in df.columns.values:\n",
    "            if feature != label_name:\n",
    "                # select different values\n",
    "                values = df[feature].unique()\n",
    "                # split data\n",
    "                subsets = split_df(df, feature, values)\n",
    "                # calculate entropy gain\n",
    "                gain = current_entropy\n",
    "                for value, subset in subsets.iteritems():\n",
    "                    p = float(len(subset.index))/len(df.index)\n",
    "                    gain -= p*entropy(subset, label_name)\n",
    "                # update if necessary\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_criteria = feature\n",
    "                    best_sets = subsets\n",
    "        return best_criteria, best_sets\n",
    "\n",
    "    def build_tree(self, df, label_name, max_depth=None):\n",
    "        \"\"\"\n",
    "        build the decision tree\n",
    "        \"\"\"\n",
    "        if len(df.index) == 0 or max_depth == 0:\n",
    "            return None\n",
    "        feature, subsets = self.find_best_criteria(df, label_name)\n",
    "        if feature is None:\n",
    "            return None\n",
    "        new_depth = None if max_depth is None else max_depth-1\n",
    "\n",
    "        children = {}\n",
    "        for value, subset in subsets.iteritems():\n",
    "            children[value] = self.build_tree(subset, label_name, new_depth)\n",
    "        return node(feature, label_counts(df, label_name), children)\n",
    "    \n",
    "    def train(self, df, label_name, max_depth=None):\n",
    "        \"\"\"\n",
    "        trains classifier with a given dataset\n",
    "        \"\"\"\n",
    "        self.tree = self.build_tree(df, label_name, max_depth)\n",
    "        \n",
    "    def predict(self, tree, row):\n",
    "        \"\"\"\n",
    "        predict the result of a given a data row\n",
    "        \"\"\"\n",
    "        if tree is None:\n",
    "            raise Exception(\"This model was not trained yet. Please train the decision tree before calling the predict method.\")\n",
    "        if (not tree.children or \n",
    "                row[tree.feature] not in tree.children or \n",
    "                tree.children[row[tree.feature]] is None):\n",
    "            return get_prediction(tree.counts)\n",
    "\n",
    "        return self.predict(tree.children[row[tree.feature]], row)\n",
    "    \n",
    "    def score(self, df, labels, label_name):\n",
    "        \"\"\"\n",
    "        returns the confusion matrix for the classifier in a given test set\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise Exception(\"This model was not trained yet. Please train the decision tree before calling the predict method.\")\n",
    "        # creating the confusion matrix\n",
    "        confusion_matrix = {}\n",
    "        for label in labels:\n",
    "            label_dict = {}\n",
    "            for other_label in labels:\n",
    "                label_dict[other_label] = 0\n",
    "            confusion_matrix[label] = label_dict\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            prediction = self.predict(self.tree, row)\n",
    "            confusion_matrix[prediction][row[label_name]] += 1\n",
    "\n",
    "        return confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our tree\n",
    "\n",
    "Let's build a simple tree with maximum depth 2 to make sure everything is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Building tree. This may take a while...\"\n",
    "columns = ['age', 'genre', 'gender', 'ocupation', 'rating']\n",
    "decision_tree_clf = TreeClassifier()\n",
    "decision_tree_clf.train(train_set[columns], 'rating', 2)\n",
    "print \"Tree built. Predicting ratings for all movies seen by users 6041 and 6042...\"\n",
    "rows = data.ix[data['user_id'].isin(['6041','6042'])]\n",
    "for _, row in rows.iterrows():\n",
    "    prediction = decision_tree_clf.predict(decision_tree_clf.tree, row)\n",
    "    print \"Tree prediction for user\", row['user_id'], \"and movie\", row['movie_id'] + \":\", str(prediction) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring performance\n",
    "\n",
    "Lets build some usefull functions to better measure the performance of a classifier. First of all, \n",
    "we'll build a function to plot the confusion matrix in a more user friendly display let's test this with the confusion matrix from the classifier we built above, for the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(matrix_in, classes, cmap=plt.cm.Blues):\n",
    "    classes.sort()\n",
    "    matrix = []\n",
    "    for s_class in classes:\n",
    "        row = []\n",
    "        for o_class in classes:\n",
    "            row.append(matrix_in[o_class][s_class])\n",
    "        matrix.append(row)\n",
    "    cm = np.array(matrix, dtype=int)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks,classes)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "labels = data['rating'].unique()\n",
    "print \"Predicting ratings for test set. This may take a while...\"\n",
    "confusion_matrix = decision_tree_clf.score(test_set[columns], labels, 'rating')\n",
    "print \"Done. Plotting the confusion matrix\"\n",
    "plot_confusion_matrix(confusion_matrix, labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating scores\n",
    "\n",
    "We will now build a function that, given a confusion matrix, computes the accuracy, mean square error and Kappa score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_statistics(confusion_matrix, labels): \n",
    "    \"\"\"\n",
    "    compute accuracy, mean square error and kappa \n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    rights = 0.0\n",
    "    error = 0.0\n",
    "    true_totals = np.zeros(len(labels)+1)\n",
    "    predict_totals = np.zeros(len(labels)+1)\n",
    "    for true_label in labels:\n",
    "        for predicted_label in labels:\n",
    "            current_val = confusion_matrix[predicted_label][true_label]\n",
    "            total += current_val\n",
    "            error += abs(true_label-predicted_label)**2 * current_val\n",
    "            predict_totals[int(predicted_label)] += current_val\n",
    "            true_totals[int(true_label)] += current_val\n",
    "        rights += confusion_matrix[true_label][true_label]\n",
    "    accuracy = rights/total\n",
    "    error /= total\n",
    "    \n",
    "    ef = 0.0\n",
    "    for label in labels:\n",
    "        ef += true_totals[int(label)]*predict_totals[int(label)]/total\n",
    "    K = (rights-ef)/(total-ef)\n",
    "    return accuracy, error, K\n",
    "\n",
    "accuracy, error, kappa = get_statistics(confusion_matrix, labels)\n",
    "print \"Accuracy:\", accuracy\n",
    "print \"Mean Square Error:\", error\n",
    "print \"Kappa score:\", kappa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best classifier\n",
    "\n",
    "Using the validation set, we will now find the best depth for the tree. We should expect large trees to overfit and small trees to underfit. Let's see how the means square error changes with the maximum depth of the tree and use that to get the best classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_best_depth(train_set, validation_set, depths, columns=None):\n",
    "    \"\"\"\n",
    "    finds the depth that scores the most in the validation set, \n",
    "    using least Kappa score as criteria\n",
    "    \"\"\"\n",
    "    print \"Finding best depth. This may take a while...\"\n",
    "    best_depth = None\n",
    "    best_clf = None\n",
    "    best_kappa =  float('inf')\n",
    "    if columns is not None:\n",
    "        train_set = train_set[columns]\n",
    "        validation_set = validation_set[columns]\n",
    "    for depth in depths:\n",
    "        print \"Training classifier with depth \" + str(depth) + \"...\"\n",
    "        clf = TreeClassifier()\n",
    "        clf.train(train_set, 'rating', depth)\n",
    "        confusion_matrix = clf.score(validation_set, labels, 'rating')\n",
    "        _, _, kappa = get_statistics(confusion_matrix, labels)\n",
    "        print \"Mean quadratic error for depth = \" + str(depth) + \": \" + str(error)\n",
    "        if kappa < best_kappa:\n",
    "            best_kappa = kappa\n",
    "            best_depth = depth\n",
    "            best_clf = clf\n",
    "    return best_depth, best_clf\n",
    "\n",
    "best_depth, best_clf = find_best_depth(train_set, validation_set, [2], columns)\n",
    "\n",
    "confusion_matrix = best_clf.score(test_set[columns], labels, 'rating')\n",
    "accuracy, error, kappa = get_statistics(confusion_matrix, labels)\n",
    "\n",
    "print \"======================================\"\n",
    "print \"Best tree depth:\", best_depth\n",
    "print \"Accuracy on test set: \" + str(accuracy)\n",
    "print \"Mean quadratic error on test set: \" + str(error)\n",
    "print \"Kappa score on test set: \" + str(kappa)\n",
    "print \"Plotting confusion matrix\"\n",
    "plot_confusion_matrix(confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A second approach\n",
    "\n",
    "While this first approach worked, the scores show there is still room for improvement. We now propose to use, instead of the movie genres, their id. In this approach, we can drop the genre features, which will save us some computational cost, in exchange for the id feature, which will be more computationally expensive. For each tree, we will be using only the features: **user gender**, **user age**, **user ocupation** and **movie_id**. Since there are 3952 different movies (as opposed to the 263 gender combinations we previously used), we should expect larger trees and higher computational costs for both creating the tree and predicting. \n",
    "\n",
    "Lets build this second approach and see how it scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns2 = ['age', 'gender', 'ocupation', 'movie_id', 'rating']\n",
    "best_depth2, best_clf2 = find_best_depth(train_set, validation_set, [2], columns2)\n",
    "\n",
    "confusion_matrix2 = best_clf2.score(test_set[columns2], labels, 'rating')\n",
    "accuracy2, error2, kappa2 = get_statistics(confusion_matrix2, labels)\n",
    "\n",
    "print \"======================================\"\n",
    "print \"Best tree depth:\", best_depth2\n",
    "print \"Accuracy on test set: \" + str(accuracy2)\n",
    "print \"Mean quadratic error on test set: \" + str(error2)\n",
    "print \"Kappa score on test set: \" + str(kappa2)\n",
    "print \"Plotting confusion matrix\"\n",
    "plot_confusion_matrix(confusion_matrix2, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Average Classifier\n",
    "\n",
    "Finally, we compare the built decision tree classifiers with the Movie Average Classifier, which predicts, for every movie, the mean of all ratings for that movie. Let's see how this classifier scores on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def movie_avg_train_and_predict(train_set, test_set, data, columns):\n",
    "    movies = test_set['movie_id'].unique()\n",
    "    _test_set = test_set[columns]\n",
    "    _train_set = train_set[columns]\n",
    "    _data = data[columns]\n",
    "    \n",
    "    movie_avg_cm = {}\n",
    "    for label in labels:\n",
    "        label_dict = {}\n",
    "        for other_label in labels:\n",
    "            label_dict[other_label] = 0\n",
    "        movie_avg_cm[label] = label_dict\n",
    "\n",
    "    i = 0\n",
    "    for movie in movies:\n",
    "        i += 1\n",
    "        if i % 200 == 0:\n",
    "            print \"Calculated score for\", str(i), \"of\", str(len(movies)), \"movies.\"\n",
    "        counts = label_counts(_test_set.ix[data['movie_id'] == movie], 'rating')\n",
    "        train_counts = label_counts(_train_set.ix[_data['movie_id'] == movie], 'rating')\n",
    "        prediction = get_prediction(train_counts)\n",
    "        for true_label, count in counts.iteritems():\n",
    "            movie_avg_cm[prediction][true_label] += count\n",
    "    return movie_avg_cm\n",
    "\n",
    "print \"Calculating score. This may take a while...\"\n",
    "columns_movie_avg = ['movie_id', 'rating']\n",
    "movie_avg_cm = movie_avg_train_and_predict(train_set, test_set, data, columns_movie_avg)\n",
    "print \"Done. Plotting confusion matrix.\"\n",
    "plot_confusion_matrix(movie_avg_cm, labels)\n",
    "(movie_avg_accuracy, movie_avg_error, movie_avg_kappa) = get_statistics(movie_avg_cm, labels)\n",
    "print \"======================================\"\n",
    "print \"Accuracy on test set: \" + str(movie_avg_accuracy)\n",
    "print \"Mean quadratic error on test set: \" + str(movie_avg_error)\n",
    "print \"Kappa score on test set: \" + str(movie_avg_kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final comparissons\n",
    "\n",
    "Finally, we compare scores for all the approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "accuracies = [accuracy, movie_avg_accuracy, accuracy2]\n",
    "errors = [error, movie_avg_error, error2]\n",
    "kappas = [kappa, movie_avg_kappa, kappa2]\n",
    "names = ['First approach', 'Movies average', 'Second approach']\n",
    "colors = ['g','b','r']\n",
    "\n",
    "def plot_statistic (statistic, names, colors, statistic_name):\n",
    "    \"\"\"\n",
    "    plot statistics\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    bar_width = 0.1\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "    for index in range(len(statistic)):\n",
    "        rect = plt.bar(0.075+bar_width*index, statistic[index], bar_width,\n",
    "                      alpha=opacity,\n",
    "                      color=colors[index],\n",
    "                      error_kw=error_config,\n",
    "                      label=names[index])\n",
    "    plt.xlabel('Approaches')\n",
    "    plt.ylabel(statistic_name)\n",
    "    plt.title(statistic_name + \" by approach\")\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.tick_params(axis='x', which='both',bottom='off', top='off', labelbottom='off')\n",
    "    plt.show()\n",
    "    \n",
    "plot_statistic(accuracies, names, colors, 'Accuracy')\n",
    "plot_statistic(errors, names, colors, 'Mean Square Errors')\n",
    "plot_statistic(kappas, names, colors, 'Kappa scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lastly and just for fun, we compare the confusion matrix and scores for the 3 classifiers, using data from the movies rated by the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_avg_columns = ['movie_id', 'rating']\n",
    "columns = ['age', 'genre', 'gender', 'ocupation', 'rating']\n",
    "columns2 = ['age', 'gender', 'ocupation', 'movie_id', 'rating']\n",
    "test_set_personal = data.ix[data['user_id'].isin(['6041','6042'])]\n",
    "\n",
    "confusion_matrix = best_clf.score(test_set_personal[columns], labels, 'rating')\n",
    "confusion_matrix2 = best_clf2.score(test_set_personal[columns2], labels, 'rating')\n",
    "movie_avg_cm = movie_avg_train_and_predict(train_set, test_set_personal, data, movie_avg_columns)\n",
    "\n",
    "(accuracy, error, kappa) = get_statistics(confusion_matrix, labels)\n",
    "(accuracy2, error2, kappa2) = get_statistics(confusion_matrix2, labels)\n",
    "(movie_avg_accuracy, movie_avg_error, movie_avg_kappa) = get_statistics(movie_avg_cm, labels)\n",
    "\n",
    "kappas = [kappa, movie_avg_kappa, kappa2]\n",
    "accuracies = [accuracy, movie_avg_accuracy, accuracy2]\n",
    "errors = [error, movie_avg_error, error2]\n",
    "\n",
    "plot_statistic(accuracies, names, colors, 'Accuracy')\n",
    "plot_statistic(errors, names, colors, 'Mean Square Errors')\n",
    "plot_statistic(kappas, names, colors, 'Kappa scores')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
